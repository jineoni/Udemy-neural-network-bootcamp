{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Linear(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2203,  0.0538, -0.1189,  0.3283, -0.2518],\n",
       "        [-0.1706, -0.4330,  0.3557,  0.2809, -0.3913],\n",
       "        [-0.0950, -0.0915, -0.3074,  0.0701, -0.1901],\n",
       "        [ 0.2460,  0.0159, -0.1827,  0.4065, -0.2886],\n",
       "        [-0.2148, -0.1444, -0.1781, -0.0085,  0.0493]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Are initialized by some technique, which is usually the Xavier initialization technique.\n",
    "layer.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Uniform Initialization**\n",
    "``torch.nn.init.uniform_(tensor, a=0.0, b=1.0)``\n",
    "- `tensor`: an n-dimensional tensor\n",
    "- `a`: the lower bound of the uniform distribution\n",
    "- `b`: the upper bound of the uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1.3863, 0.0039, 0.9148, 1.4397, 1.2851],\n",
       "        [1.6788, 0.7291, 0.9323, 0.1349, 0.5874],\n",
       "        [2.6834, 1.0229, 0.7339, 2.3156, 0.6781],\n",
       "        [1.2110, 0.8513, 2.3238, 2.9968, 2.7035],\n",
       "        [1.5223, 2.2078, 2.3442, 0.6581, 0.5872]], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.uniform_(layer.weight, a=0.0, b=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Normal Initialization**\n",
    "``torch.nn.init.normal_(tensor, mean=0.0, std=1.0)``\n",
    "- `tensor`: an n-dimensional tensor\n",
    "- `mean`: the mean of the normal distribution\n",
    "- `std`: the standard deviation of the normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3308,  0.5445,  0.3410,  0.9435,  0.5938],\n",
       "        [-0.8397, -0.7162, -0.9159, -0.0985, -0.5954],\n",
       "        [ 0.4796, -0.1454, -0.8953, -0.3458,  1.8096],\n",
       "        [ 0.6664,  1.4431,  0.2402,  0.5557, -0.8578],\n",
       "        [ 0.3430, -0.6350, -0.0654,  0.2488,  1.0524]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.normal_(layer.weight, mean=0.0, std=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Constant Initialization**\n",
    "Usually it's very wrong to initialize the weights to a constant value. However, this is useful when you want to initialize your biases.\n",
    "``torch.nn.init.constant_(tensor, val)``\n",
    "- `tensor`: an n-dimensional tensor\n",
    "- `val`: the value to fill the tensor with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.constant_(layer.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.zeros_(layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Xavier Initialization**\n",
    "``torch.nn.init.xavier_uniform_(tensor, gain=1.0)``\n",
    "\n",
    "The resulting tensor will have values sampled from $U(-a, a)$ where $a = gain \\times \\sqrt(\\frac{6}{n_{in} + n_{out}})$.\n",
    "\n",
    "``torch.nn.init.xavier_normal_(tensor, gain=1.0)``\n",
    "\n",
    "The resulting tensor will have values sampled from $N(0, std^2)$ where $std = gain \\times \\sqrt(\\frac{2}{n_{in} + n_{out}})$.\n",
    "- `tensor`: an n-dimensional tensor\n",
    "- `gain`: an optional scaling factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ShinJiyoon\\AppData\\Local\\Temp\\ipykernel_1420\\218849935.py:1: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(layer.weight, gain=1.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1719,  0.4397, -0.4870, -0.2959,  0.4742],\n",
       "        [ 0.6008, -0.0152, -0.1795,  0.1005,  0.0235],\n",
       "        [ 0.4696, -0.0157, -0.4571, -0.2879, -0.5624],\n",
       "        [ 0.1625,  0.0645, -0.5367,  0.0483, -0.0528],\n",
       "        [-1.0488, -0.0843,  0.5984,  0.0740,  0.0256]], requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.xavier_normal(layer.weight, gain=1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
